{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40352436-8107-4e8e-bfa1-21fd1741cebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://62216fce4c770f92d2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://62216fce4c770f92d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da09c52a88548f285657fced86e5fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479487ee8752419ca400b6c446d3f2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "import time\n",
    "import subprocess\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Initialize a global variable to store the Gradio interface\n",
    "loaded_models = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "selected_model = None\n",
    "model1 = \"merged_falcon-7b_8bit\"\n",
    "model2 = \"falcon-7b\"\n",
    "\n",
    "def flush_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    tokenizer = model = None \n",
    "\n",
    "def load_model(model_name):\n",
    "    flush_gpu_memory()\n",
    "    global selected_model\n",
    "    selected_model = model_name\n",
    "    if selected_model == None:\n",
    "        yield f\"No model selected\"\n",
    "        print(\"No model selected\")\n",
    "        return\n",
    "    else:   \n",
    "        if selected_model != []:\n",
    "            yield f\"Selected model is `{selected_model}`\"\n",
    "            time.sleep(2)\n",
    "            yield f\"Loading `{selected_model}`... into {device}\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "            loaded_models[model_name] = {\"tokenizer\": tokenizer, \"model\": model}\n",
    "            yield f\"Successfully loaded `{selected_model}` into {device}.\"\n",
    "        else:\n",
    "            yield f\"Failed to load model `{selected_model}`. Please select a model and press Reload Model button.\"\n",
    "\n",
    "def generate_response(input_text, _):\n",
    "    return _generate_response(input_text) #, which is a SQL statement that can solve the Task.\n",
    "\n",
    "def generate_response(input_text):\n",
    "        prompt = f\"\"\"\n",
    "        # Instruction:\n",
    "        Use the context below to produce the result\n",
    "        # context:\n",
    "        {input_text}\n",
    "        # result:\n",
    "        \"\"\"\n",
    "        if selected_model == None or selected_model == []:\n",
    "            return \"Model not loaded. Select a model in the dropdown menu and click the 'Reload Model' button to load a model.\"\n",
    "        if not input_text:\n",
    "            return \"Please enter some text in the input field before submitting.\"\n",
    "        tokenizer = loaded_models[selected_model][\"tokenizer\"]\n",
    "        model = loaded_models[selected_model][\"model\"]\n",
    "        #input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n",
    "        #response = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2,attention_mask=attention_mask)\n",
    "        #response_text = tokenizer.decode(response[0], skip_special_tokens=True).replace(input_text, \"\").strip() #.replace removes the input text from the generated output\n",
    "        response = model.generate(input_ids, max_new_tokens=200, do_sample=True, top_p=0.9,temperature=0.5,attention_mask=attention_mask)\n",
    "        response_text = tokenizer.decode(response[0], skip_special_tokens=True).replace(prompt, \"\").strip() #.replace removes the input text from the generated output\n",
    "        return response_text\n",
    "    \n",
    "def run_os_command_nvidia_smi():\n",
    "    current_time = datetime.datetime.now(pytz.timezone('Asia/Singapore')).strftime(\"%Y-%m-%d %H:%M:%S %Z\")  \n",
    "    command = \"nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv,noheader,nounits\"\n",
    "    process = subprocess.Popen(\n",
    "        command,\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True\n",
    "    )\n",
    "    output_lines = process.stdout.read().splitlines()\n",
    "\n",
    "    # Construct a table in HTML\n",
    "    table_html = \"<table>\"\n",
    "\n",
    "    # Add headers\n",
    "    table_html += \"<tr><th>GPU</th><th>Memory Used</th><th>Memory Total</th><th>Memory Used %</th></tr>\"\n",
    "\n",
    "    for line in output_lines:\n",
    "        columns = line.split(',')\n",
    "        if len(columns) != 0:\n",
    "            gpu_index, memory_used, memory_total = columns[:3]\n",
    "            gpu_index = f\"GPU {gpu_index}\"\n",
    "            memory_used = f\"{memory_used} MiB\"\n",
    "            memory_total = f\"{memory_total} MiB\"\n",
    "\n",
    "            # Calculate memory used percentage\n",
    "            memory_used_value = float(memory_used.split()[0])\n",
    "            memory_total_value = float(memory_total.split()[0])\n",
    "            memory_used_percentage = f\"{(memory_used_value / memory_total_value * 100):.2f}%\"\n",
    "        else:\n",
    "            gpu_index = memory_used = memory_total = memory_used_percentage = \"N/A\"\n",
    "\n",
    "        table_html += f\"<tr><td>{gpu_index}</td><td>{memory_used}</td><td>{memory_total}</td><td>{memory_used_percentage}</td></tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    result_html = f\"<h4>GPU Status ({current_time}):</h4>\"\n",
    "    result_html += table_html\n",
    "    return result_html\n",
    "\n",
    "    \n",
    "def create_ui():\n",
    "    gr.HTML(f\"<h1>TextAI using Fine-tuned '{model1}' Model with Custom Dataset</h1>\")\n",
    "    with gr.Tab(\"AI Text Generator\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                model_selected = gr.Dropdown(choices=[model1,model2], label='Select a GenAI Model')        \n",
    "                reload_button = gr.Button(\"Reload Model\", variant=\"secondary\")\n",
    "                status_message = gr.Label(label=\"Model Status\")\n",
    "                inp = model_selected   \n",
    "                reload_button.click(load_model, inputs=inp, outputs=status_message)\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gpuinfo = gr.HTML(lambda: run_os_command_nvidia_smi())\n",
    "                        reload_button.click(run_os_command_nvidia_smi, outputs=gpuinfo)\n",
    "                        gpu_button = gr.Button(\"Refresh GPU Status\", variant=\"secondary\")\n",
    "                        gpu_button.click(run_os_command_nvidia_smi, outputs=gpuinfo)\n",
    "                        with gr.Row():\n",
    "                            with gr.Column():\n",
    "                                global iface2  \n",
    "                                iface2 = gr.Interface(\n",
    "                                fn=generate_response,\n",
    "                                inputs=\"text\",  \n",
    "                                outputs=\"text\",\n",
    "                                allow_flagging=\"never\",\n",
    "                                title=\"Test the Loaded Model:\",\n",
    "                                #description=\"Enter a message to chat with the loaded model.\",\n",
    "                                examples=[\n",
    "                                [\"CREATE TABLE book (Title VARCHAR, Writer VARCHAR). What are the titles of the books whose writer is not Dennis Lee?\"],\n",
    "                                [\"CREATE TABLE trip (bus_stop VARCHAR, duration INTEGER). List all the bus stops from which a trip of duration below 100 started.\"],\n",
    "                                [\"def hello_world():\"],\n",
    "                                ],    \n",
    "                                )          \n",
    "\n",
    "mytheme = gr.themes.Soft().set(\n",
    "    button_secondary_background_fill=\"#ade6d8\",\n",
    "    button_secondary_background_fill_hover=\"#AAAAAA\",\n",
    "    button_primary_background_fill=\"#2c9178\",\n",
    "    button_primary_background_fill_hover=\"#AAAAAA\",\n",
    "    button_shadow=\"*shadow_drop_lg\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(theme=mytheme) as demo:\n",
    "    create_ui()\n",
    "demo.queue()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7d7d0-f7b3-46e2-96b5-5fb34a46cb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
