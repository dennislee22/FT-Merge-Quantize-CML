{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2013ab-df39-44bc-963f-9ab2b6e0eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158d30d4-593d-4093-8e14-a67b3c7453a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d18912109bf4346b29740712909dd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_name = \"gptq_bloom-7b1_Q4bit\"\n",
    "device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(quantized_name, device_map=device_map)\n",
    "tokenizer = AutoTokenizer.from_pretrained(quantized_name,device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4665daf4-68e4-4e2c-b348-6f68924c95c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_precision(model):\n",
    "  dtypes = {}\n",
    "  for _, p in model.named_parameters():\n",
    "      dtype = p.dtype\n",
    "      if dtype not in dtypes:\n",
    "          dtypes[dtype] = 0\n",
    "      dtypes[dtype] += p.numel()\n",
    "  total = 0\n",
    "  for k, v in dtypes.items():\n",
    "      total += v\n",
    "  for k, v in dtypes.items():\n",
    "      print(f\"{k}, {v / 10**6:.4f} M, {v / total*100:.2f} %\")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "  # Count the total parameters\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  print(f\"Total parameters: {total_params/10**6:.4f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a91638-9362-4a65-b32d-9f1732797a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 Memory Footprint: 7861.3594 MB\n",
      "\n",
      "Data types:\n",
      "torch.float16, 1028.1124 M, 100.00 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{device_map} Memory Footprint: {model.get_memory_footprint() / 1024**2:.4f} MB\")\n",
    "print(\"\\nData types:\")\n",
    "print_param_precision(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98b5cf2-52c0-41a6-9130-4632ee8ea2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mytask=\"CREATE TABLE trip (bus_stop VARCHAR, duration INTEGER), list all the bus stops from which a trip of duration below 100 started.\"\n",
    "mytask=\"CREATE TABLE book (Title VARCHAR, Writer VARCHAR). What are the titles of the books whose writer is not Dennis Lee?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "# Instruction:\n",
    "Use the context below to produce the result\n",
    "# context:\n",
    "{mytask}\n",
    "# result:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7ffba0-942e-4188-a86d-63535fc96355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Prompt:\n",
      "# Instruction:\n",
      "Use the context below to produce the result\n",
      "# context:\n",
      "CREATE TABLE book (Title VARCHAR, Writer VARCHAR). What are the titles of the books whose writer is not Dennis Lee?\n",
      "# result:\n",
      "\n",
      "--------------------------------------\n",
      "Quantized Model Result :\n",
      "SELECT Title FROM book WHERE Writer <> \"Dennis Lee\"\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "input_id1 = tokenizer.encode(prompt, return_tensors=\"pt\").to(device_map)\n",
    "attention_mask1 = torch.ones(input_id1.shape, dtype=torch.long).to(device_map)\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"Prompt:{prompt}\")\n",
    "print(f\"--------------------------------------\")\n",
    "\n",
    "print(f\"Quantized Model Result :\")\n",
    "output = model.generate(input_ids=input_id1, do_sample=True, max_new_tokens=100, top_p=0.9,temperature=0.5,attention_mask=attention_mask1)\n",
    "print(f\"{tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cfdf0-f8df-4e4d-8cf7-afe2a3788107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
