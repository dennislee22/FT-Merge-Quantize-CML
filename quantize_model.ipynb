{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c535c4c6-b2d2-4797-a6c0-082dbb6c4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GPTQConfig, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f728e3-1ad8-41a6-a4e8-37574391ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bloom-1b1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "gptq_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e40f82-3c8f-4349-8fa6-424159d9067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f8f3ab-684c-438b-a49e-8694ad84f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cb7a9-6acb-490d-8a95-42b18dfad527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=gptq_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b397e788-ad81-4292-90d1-753aac288272",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57900475-24bd-422f-80a9-83d7892fb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b39398-afde-4fd5-9038-bbd8cba05f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_precision(model):\n",
    "  dtypes = {}\n",
    "  for _, p in model.named_parameters():\n",
    "      dtype = p.dtype\n",
    "      if dtype not in dtypes:\n",
    "          dtypes[dtype] = 0\n",
    "      dtypes[dtype] += p.numel()\n",
    "  total = 0\n",
    "  for k, v in dtypes.items():\n",
    "      total += v\n",
    "  for k, v in dtypes.items():\n",
    "      print(f\"{k}, {v / 10**6:.4f} M, {v / total*100:.2f} %\")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "  # Count the total parameters\n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  print(f\"Total parameters: {total_params/10**6:.4f} M\")\n",
    "\n",
    "  # Count the trainable parameters\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  print(f\"Trainable parameters: {trainable_params/10**6:.4f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2136882-a75f-49ba-a492-9fa8abb69f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 Memory Used: 1383.9258 MB\n",
      "\n",
      "Parameters:\n",
      "Total parameters: 1065.3143 M\n",
      "Trainable parameters: 385.5053 M\n",
      "\n",
      "Data types:\n",
      "torch.float16, 385.8371 M, 36.22 %\n",
      "torch.int8, 679.4772 M, 63.78 %\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device} Memory Used: {model.get_memory_footprint() / 1024**2:.4f} MB\")\n",
    "print(\"\\nParameters:\")\n",
    "print_trainable_parameters(model)\n",
    "print(\"\\nData types:\")\n",
    "print_param_precision(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2d442b-ea16-4653-93f2-f5711d38cbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gptq-bloom-1b1/tokenizer_config.json',\n",
       " 'gptq-bloom-1b1/special_tokens_map.json',\n",
       " 'gptq-bloom-1b1/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"gptq-bloom-1b1\")\n",
    "tokenizer.save_pretrained(\"gptq-bloom-1b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c55f2d-5f40-4978-9542-2b4d53610500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 Memory Used: 1383.9258 MB\n",
      "\n",
      "Parameters:\n",
      "Total parameters: 1065.3143 M\n",
      "Trainable parameters: 385.5053 M\n",
      "\n",
      "Data types:\n",
      "torch.float16, 385.8371 M, 36.22 %\n",
      "torch.int8, 679.4772 M, 63.78 %\n"
     ]
    }
   ],
   "source": [
    "new_model = \"gptq-bloom-1b1\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"{device} Memory Used: {model.get_memory_footprint() / 1024**2:.4f} MB\")\n",
    "print(\"\\nParameters:\")\n",
    "print_trainable_parameters(model)\n",
    "print(\"\\nData types:\")\n",
    "print_param_precision(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c75b91-1543-49c6-90ec-1085763779d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'named_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2399/3893022167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Parameter name: {name}, Data type: {param.dtype}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'named_parameters'"
     ]
    }
   ],
   "source": [
    "#load model into memory\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b19897-f1ff-4717-b576-b43586737618",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in new_model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Data type: {param.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
